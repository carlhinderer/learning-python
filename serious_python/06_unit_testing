--------------------------------------------------------------------
CHAPTER 6 - UNIT TESTING
--------------------------------------------------------------------

- Simple Tests

    - Tests should be a submodule of your main package, and should be shipped along with the 
        source code.  This allows the tests to be reused by whoever downloads the module, and
        prevents the tests from being accidentally installed as a top-level module.


    - Using a hierarchy of tests that mimics your code hierarchy will make the tests much more
        managable.  So, tests covering 'mylib/foobar.py' should be stored in 
        'mylib/tests/test_foobar.py'.


    - Here is the simplest possible unit test:

        # Passing test
        def test_true():
            assert True

        # Failing test
        def test_false():
            assert False


        # Run the test
        $ pytest -v test_true.py


    - This simple testing strategy works well for many small projects, requiring nothing more than
        pytest and assertion tests.



- Skipping Tests

    - If a test cannot be run, it makes sense to skip the test.  For instance, you may want to run
        a test conditionally based on whether a particular library is present.


        import pytest

        try:
            import mylib
        except ImportError:
            mylib = None

        @pytest.mark.skip("Do not run this")
        def test_fail():
            assert False

        @pytest.mark.skipif(mylib is None, reason="mylib is not available")
        def test_mylib():
            assert mylib.foobar() == 42

        def test_skip_at_runtime():
            if True:
                pytest.skip("Finally I don't want to run it")



- Running Particular Tests

    - To run only specific tests:

        # Run a single file
        $ pytest -v examples/test_skip.py

        # Run tests whose method names match a pattern
        $ pytest -v examples/test_skip.py -k test_fail
 

    - We can also run only tests that are marked with a decorator:

        # test_mark.py
        import pytest

        @pytest.mark.dicttest
        def test_something():
            a = ['a', 'b']
            assert a == a

        def test_something_else():
            assert False


        # Run the tests marked with an attribute
        $ pytest -v test_mark.py -m dicttest

        # Run the tests not marked
        $ pytest -v test_mark.py -m 'not dicttest'



- Running Tests in Parallel

    - Test suites can take a long time to run.  By default, pytest runs all tests serially, in
        undefined order.

      We can speed things up by running tests on multiple CPUs at the same time.  We start by 
        downloading the 'pytest-xdist' plugin (installed with pip), then we specify the number
        of parallel processes we want to have.


        # Run 4 parallel processes
        $ pytest -n 4
        $ pytest --numprocesses 4

        # Run (# of cpus on machine) processes
        $ pytest -n auto



- Creating Objects Used in Tests with Fixtures

    - Fixtures are components that are set up before a test and deleted once the test is
        done.  With pytest, fixtures are defined as simple functions.


    - Here, we specify the setup in the fixture:

        import pytest

        @pytest.fixture
        def database():
            return <some database connection>

        def test_insert(database):
            database.insert(123)


    - Here, we specify both the setup and teardown in the fixture:

        import pytest

        @pytest.fixture
        def database():
            db = <some database connection>
            yield db
            db.close()

        def test_insert(database):
            database.insert(123)


    - Since closing the database connection on each test might be costly, we can reuse the same 
        connection for all the tests.  

      Here, the setup for the fixture runs once before all the tests and the teardown runs once
        after all the tests are finished:

        import pytest

        @pytest.fixture(scope='module')
        def database():
            db = <some database connection>
            yield db
            db.close()


    - The 'autouse' option will cause the fixture to be run before and after each test, without 
        having to specify it as an argument in the tests that use it.

        import os, pytest

        @pytest.fixture(autouse=True)
        def change_user_env():
            curuser = os.environ.get("USER")
            os.environ["USER"] = "foobar"
            yield
            os.environ["USER"] =  curuser

        def test_user():
            assert os.getenv("USER") == "foobar"



- Running Test Scenarios

    - Let's say we want to run an entire test suite against different drivers.   For instance, we
        might have an abstract class called the 'Storage API' in our application.  Any Python 
        class can implement this abstract base and register itself to become a driver.  The
        package loads the configured storage driver and uses it to store and retrieve data.


    - We need a class of unit tests that runs against each driver to make sure all drivers conform
        to what clients expect.  An easy way to achieve this is with 'parameterized fixtures' which
        will run all the tests for each of the defined parameters.


        # Run tests for mysql and postgres
        import myapp, pytest

        @pytest.fixture(params=['mysql', 'postgresql'])
        def database(request):
            d = myapp.driver(request.param)
            d.start()
            yield d
            d.stop()

        def test_insert(database):
            database.insert("somedata")



- Controlled Tests Using Mocking

    - Mock objects are simulated objects that mimic the behavior of real application objects.  They
        allow you to create environments that precisely describe the conditions you want to test for.


    - The standard library for creating mock objects in Python is 'mock'.  As of Python 3.3, it has
        been merged into the standard library as 'unittest.mock'.

        # Create a mock with an attribute
        >>> from unittest import mock
        >>> m = mock.Mock()
        >>> m.some_attribute = 'hello world'
        >>> m.some_attribute
        'hello world'

        # Add a method
        >>> m.some_method.return_value = 42
        >>> m.some_method()
        42


    - Dynamically created methods can also have side effects.

        >>> m = mock.Mock()
        >>> def print_hello():
                print('hello world!')
                return 43

        >>> m.some_method.side_effects = print_hello
        >>> m.some_method()
        hello world!
        43

        >>> m.some_method.call_count
        1


    - The 'mock' library uses the action/assertion pattern.  

        # Call a method
        >>> from unittest import mock
        >>> m = mock.Mock()
        >>> m.some_method('foo', 'bar')

        # Check that method was called with arguments
        >>> m.some_method.assert_called_once_with('foo', 'bar')
        >>> m.some_method.assert_called_once_with('foo', mock.ANY)

        # This fails, since it wasn't called with these arguments
        >>> m.some_method.assert_called_once_with('foo', 'baz')


    - The 'mock' library can also be used to patch some function, method, or object from an
        external module.  

        # Replace os.unlink() 
        >>> from unittest import mock
        >>> import os

        >>> def fake_os_unlink(path):
                raise IOError("Testing!")

        >>> with mock.patch('os.unlink', fake_os_unlink):
                os.unlink('foobar')
        Testing!



- Example - Simulating a Web Server with Mocks

    - This test suite searches for all instances of the string 'Python is a programming langage'
        on the 'http://python.org/' web page.  

      Since we have no way to test scenarios in which the string isn't there, we create mocked
        replies with fake pages that don't contain the string.


        from unittest import mock
    
        import pytest
        import requests
      
        class WhereIsPythonError(Exception):
            pass
      
        def is_python_still_a_programming_language():
            try:
                r = requests.get("http://python.org")
            except IOError:
                pass
            else:
                if r.status_code == 200:
                    return 'Python is a programming language' in r.content
            raise WhereIsPythonError("Something bad happened")
      
        def get_fake_get(status_code, content):
            m = mock.Mock()
            m.status_code = status_code
            m.content = content
      
            def fake_get(url):
                return m
      
            return fake_get
      
        def raise_get(url):
            raise IOError("Unable to fetch url %s" % url)
      
        @mock.patch('requests.get', get_fake_get(200, 'Python is a programming language for sure'))
        def test_python_is():
            assert is_python_still_a_programming_language() is True
      
        @mock.patch('requests.get', get_fake_get(200, 'Python is no more a programming language'))
        def test_python_is_not():
            assert is_python_still_a_programming_language() is False
      
        @mock.patch('requests.get', get_fake_get(404, 'Whatever'))
        def test_bad_status_code():
            with pytest.raises(WhereIsPythonError):
                is_python_still_a_programming_language()
      
        @mock.patch('requests.get', raise_get)
        def test_ioerror():
           with pytest.raises(WhereIsPythonError):
               is_python_still_a_programming_language()
    
    
    
- Revealing Untested Code with coverage

    - The 'coverage' tool identifies whether any of your code has been missed during testing.
        It uses code analysis tools and tracing hooks to determine which lines of your code
        have been executed.

      If we use coverage in standalone mode, it can show you what code is uncovered and what
        code is dead.  

        # Install coverage
        $ pip install coverage


    - When using pytest, we use the 'pytest-cov' plugin, which generates a detailed coverage
        report.

        # Install pytest-cov
        $ pip install pytest-cov

        # Now, we pass in the source and test directories
        $ pytest --cov=gnocchiclient gnocchiclient/tests/unit

        # Generate an HTML coverage report instead of text
        $ pytest --cov=gnocchiclient gnocchiclient/tests/unit --cov-report=html


    - Some teams will set a minimum percentage of the code that needs to be covered.  The test
        suite will fail if if the threshold is not met.

        # Must be 100% covered
        $ pytest --cov=gnocchiclient gnocchiclient/tests/unit --cover-fail-under=100



- Virtual Environments

    - Once an application reaches a significant size, it will depend on many external libraries.
        Tests may not reveal problems in which:

          1. You don't have the library you need.
          2. You don't have the right version of the library you need.
          3. You need 2 different versions of the same library for different applications.


    - The solution is for each application to use its own library directory that contains all of an 
        application's dependencies.  This directory is known as a 'virtual environment'.



- Setting Up a Virtual Environment

    - The 'virtualenv' tool handles virtual environments automatically for you.  

        # Create virtual environment
        $ python3 -m venv myvenv

        # Activate the virtual environment
        $ source myenv/bin/activate

        # Deactivate the virtual environment
        $ deactivate



- Using virtualenv with tox

    - The 'tox' management tool aims to automate and standardize how tests are run in Python.
        It provides everything needed to run an entire test suite in a clean virtual environment,
        while also installing your application to check that the installation works.

        $ pip install tox


    - First, we need to create a config file 'tox.ini' in the root directory of our project, 
        beside the 'setup.py'.

        # Create a config file
        $ touch tox.ini

        # Run tox
        $ tox


    - In this instance, tox creates a virtual environment in .tox/python using the default 
        Python version.  It uses 'setup.py' to create a distribution of your package, which it
        then installs in this virtual environment.  


    - Next, we add a command to run to our 'tox.ini'.  We need to add pytest as a dependency
        that must be installed in our virutal environment.

        # tox.ini
        [testenv]
        deps=pytest
        commands=pytest


    - Now, when we run 'tox', tox recreates the environment, installs the new dependency, and
        runs the command 'pytest', which executes all the unit tests.



- Re-creating an Environment

    - Sometimes, we'll need to recreate an environment.  tox acceps a '--recreate' option that
        will rebuild the virtual environment from scratch.



- Using Different Python Versions

    - Here, we specify that we want a test environment with Python 2.1:

        # tox.ini
        [testenv]
        deps=pytest
        commands=pytest

        [testenv:py21]
        basepython=python2.1



- Integrating Other Tests

    - We can also use tox to integrate things like flake8:

        # tox.ini
        [tox]
        envlist=py35,py36,pypy,pep8
        
        [testenv]
        deps=pytest
        commands=pytest
        
        [testenv:pep8]
        deps=flake8
        commands=flake8


    - When running tox, all the environments are built and run sequentially, which can make it
        a long process.  However, since the virtual environments are isolated, nothing prevents 
        them from running sequentially.

      This is what the 'detox' package was created to do.  It runs all of the default environments
        from 'envlist' in parallel.

        $ pip install detox



- Testing Policy

    - There should always be a zero-tolerance policy against merging in code without a proper set
        of unit tests to cover it.  


    - Each of the commits you push should pass all tests.  Automating this process is even better.

      For example, we can create a workflow that relies on Gerrit (a web-based code review service)
        and Zuul (a CI and delivery service).  Each commit goes through the code review system
        provided by Gerrit, and Zuul runs the unit tests and higher-level functional tests for each
        project.


    - Travis CI is a popular tool that allows you to run tests after each push or merge or against
        pull requests that are submitted.  The testing is done post-push, but is still a fantastic
        way to track regressions.


    - We add a 'travis.yml' file to specify how tests are run:

        # travis.yml

        language: python
        python:
          - "2.7"
          - "3.6"

        # command to install dependencies
        install: "pip install -r requirements.txt --use-mirrors"

        # command to run tests
        script: pytest



- Python Coding Strategies

    1. Separate out concerns and don't do multiple things in one place.  This makes reuse natural,
         makes it easier to put test doubles in place.

    2. Take a purely functional approach when possible.  For example, in a single method either 
         calculate something or change some state, but avoid doing both.

    3. The most heinous things to test are deeply layered stacks with complex cross-layer behavioral
         dependencies.  We want to evolve the code so that the contract between layers is simple,
         predictable, and (especially for testing) replacable.