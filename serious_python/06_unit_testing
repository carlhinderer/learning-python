--------------------------------------------------------------------
CHAPTER 6 - UNIT TESTING
--------------------------------------------------------------------

- Simple Tests

    - Tests should be a submodule of your main package, and should be shipped along with the 
        source code.  This allows the tests to be reused by whoever downloads the module, and
        prevents the tests from being accidentally installed as a top-level module.


    - Using a hierarchy of tests that mimics your code hierarchy will make the tests much more
        managable.  So, tests covering 'mylib/foobar.py' should be stored in 
        'mylib/tests/test_foobar.py'.


    - Here is the simplest possible unit test:

        # Passing test
        def test_true():
            assert True

        # Failing test
        def test_false():
            assert False


        # Run the test
        $ pytest -v test_true.py


    - This simple testing strategy works well for many small projects, requiring nothing more than
        pytest and assertion tests.



- Skipping Tests

    - If a test cannot be run, it makes sense to skip the test.  For instance, you may want to run
        a test conditionally based on whether a particular library is present.


        import pytest

        try:
            import mylib
        except ImportError:
            mylib = None

        @pytest.mark.skip("Do not run this")
        def test_fail():
            assert False

        @pytest.mark.skipif(mylib is None, reason="mylib is not available")
        def test_mylib():
            assert mylib.foobar() == 42

        def test_skip_at_runtime():
            if True:
                pytest.skip("Finally I don't want to run it")



- Running Particular Tests

    - To run only specific tests:

        # Run a single file
        $ pytest -v examples/test_skip.py

        # Run tests whose method names match a pattern
        $ pytest -v examples/test_skip.py -k test_fail
 

    - We can also run only tests that are marked with a decorator:

        # test_mark.py
        import pytest

        @pytest.mark.dicttest
        def test_something():
            a = ['a', 'b']
            assert a == a

        def test_something_else():
            assert False


        # Run the tests marked with an attribute
        $ pytest -v test_mark.py -m dicttest

        # Run the tests not marked
        $ pytest -v test_mark.py -m 'not dicttest'



- Running Tests in Parallel

    - Test suites can take a long time to run.  By default, pytest runs all tests serially, in
        undefined order.

      We can speed things up by running tests on multiple CPUs at the same time.  We start by 
        downloading the 'pytest-xdist' plugin (installed with pip), then we specify the number
        of parallel processes we want to have.


        # Run 4 parallel processes
        $ pytest -n 4
        $ pytest --numprocesses 4

        # Run (# of cpus on machine) processes
        $ pytest -n auto



- Creating Objects Used in Tests with Fixtures

    - Fixtures are components that are set up before a test and deleted once the test is
        done.  With pytest, fixtures are defined as simple functions.


    - Here, we specify the setup in the fixture:

        import pytest

        @pytest.fixture
        def database():
            return <some database connection>

        def test_insert(database):
            database.insert(123)


    - Here, we specify both the setup and teardown in the fixture:

        import pytest

        @pytest.fixture
        def database():
            db = <some database connection>
            yield db
            db.close()

        def test_insert(database):
            database.insert(123)


    - Since closing the database connection on each test might be costly, we can reuse the same 
        connection for all the tests.  

      Here, the setup for the fixture runs once before all the tests and the teardown runs once
        after all the tests are finished:

        import pytest

        @pytest.fixture(scope='module')
        def database():
            db = <some database connection>
            yield db
            db.close()


    - The 'autouse' option will cause the fixture to be run before and after each test, without 
        having to specify it as an argument in the tests that use it.

        import os, pytest

        @pytest.fixture(autouse=True)
        def change_user_env():
            curuser = os.environ.get("USER")
            os.environ["USER"] = "foobar"
            yield
            os.environ["USER"] =  curuser

        def test_user():
            assert os.getenv("USER") == "foobar"



- Running Test Scenarios

    - Let's say we want to run an entire test suite against different drivers.   For instance, we
        might have an abstract class called the 'Storage API' in our application.  Any Python 
        class can implement this abstract base and register itself to become a driver.  The
        package loads the configured storage driver and uses it to store and retrieve data.


    - We need a class of unit tests that runs against each driver to make sure all drivers conform
        to what clients expect.  An easy way to achieve this is with 'parameterized fixtures' which
        will run all the tests for each of the defined parameters.


        # Run tests for mysql and postgres
        import myapp, pytest

        @pytest.fixture(params=['mysql', 'postgresql'])
        def database(request):
            d = myapp.driver(request.param)
            d.start()
            yield d
            d.stop()

        def test_insert(database):
            database.insert("somedata")



- Controlled Tests Using Mocking

    - Mock objects are simulated objects that mimic the behavior of real application objects.  They
        allow you to create environments that precisely describe the conditions you want to test for.


    - The standard library for creating mock objects in Python is 'mock'.  As of Python 3.3, it has
        been merged into the standard library as 'unittest.mock'.

        # Create a mock with an attribute
        >>> from unittest import mock
        >>> m = mock.Mock()
        >>> m.some_attribute = 'hello world'
        >>> m.some_attribute
        'hello world'

        # Add a method
        >>> m.some_method.return_value = 42
        >>> m.some_method()
        42


    - Dynamically created methods can also have side effects.

        >>> m = mock.Mock()
        >>> def print_hello():
                print('hello world!')
                return 43

        >>> m.some_method.side_effects = print_hello
        >>> m.some_method()
        hello world!
        43

        >>> m.some_method.call_count
        1


    - The 'mock' library uses the action/assertion pattern.  

        # Call a method
        >>> from unittest import mock
        >>> m = mock.Mock()
        >>> m.some_method('foo', 'bar')

        # Check that method was called with arguments
        >>> m.some_method.assert_called_once_with('foo', 'bar')
        >>> m.some_method.assert_called_once_with('foo', mock.ANY)

        # This fails, since it wasn't called with these arguments
        >>> m.some_method.assert_called_once_with('foo', 'baz')


    - The 'mock' library can also be used to patch some function, method, or object from an
        external module.  

        # Replace os.unlink() 
        >>> from unittest import mock
        >>> import os

        >>> def fake_os_unlink(path):
                raise IOError("Testing!")

        >>> with mock.patch('os.unlink', fake_os_unlink):
                os.unlink('foobar')
        Testing!



- Example - Simulating a Web Server with Mocks

    - This test suite searches for all instances of the string 'Python is a programming langage'
        on the 'http://python.org/' web page.  

      Since we have no way to test scenarios in which the string isn't there, we create mocked
        replies with fake pages that don't contain the string.


        from unittest import mock
    
        import pytest
        import requests
      
        class WhereIsPythonError(Exception):
            pass
      
        def is_python_still_a_programming_language():
            try:
                r = requests.get("http://python.org")
            except IOError:
                pass
            else:
                if r.status_code == 200:
                    return 'Python is a programming language' in r.content
            raise WhereIsPythonError("Something bad happened")
      
        def get_fake_get(status_code, content):
            m = mock.Mock()
            m.status_code = status_code
            m.content = content
      
            def fake_get(url):
                return m
      
            return fake_get
      
        def raise_get(url):
            raise IOError("Unable to fetch url %s" % url)
      
        @mock.patch('requests.get', get_fake_get(200, 'Python is a programming language for sure'))
        def test_python_is():
            assert is_python_still_a_programming_language() is True
      
        @mock.patch('requests.get', get_fake_get(200, 'Python is no more a programming language'))
        def test_python_is_not():
            assert is_python_still_a_programming_language() is False
      
        @mock.patch('requests.get', get_fake_get(404, 'Whatever'))
        def test_bad_status_code():
            with pytest.raises(WhereIsPythonError):
                is_python_still_a_programming_language()
      
        @mock.patch('requests.get', raise_get)
        def test_ioerror():
           with pytest.raises(WhereIsPythonError):
               is_python_still_a_programming_language()
    
    
    
- Revealing Untested Code with coverage

    - The 'coverage' tool identifies whether any of your code has been missed during testing.
        It uses code analysis tools and tracing hooks to determine which lines of your code
        have been executed.

      If we use coverage in standalone mode, it can show you what code is uncovered and what
        code is dead.  

        # Install coverage
        $ pip install coverage


    - When using pytest, we use the 'pytest-cov' plugin, which generates a detailed coverage
        report.

        # Install pytest-cov
        $ pip install pytest-cov

        # Now, we pass in the source and test directories
        $ pytest --cov=gnocchiclient gnocchiclient/tests/unit

        # Generate an HTML coverage report instead of text
        $ pytest --cov=gnocchiclient gnocchiclient/tests/unit --cov-report=html


    - Some teams will set a minimum percentage of the code that needs to be covered.  The test
        suite will fail if if the threshold is not met.

        # Must be 100% covered
        $ pytest --cov=gnocchiclient gnocchiclient/tests/unit --cover-fail-under=100



- Virtual Environments

    - Once an application reaches a significant size, it will depend on many external libraries.
        Tests may not reveal problems in which:

          1. You don't have the library you need.
          2. You don't have the right version of the library you need.
          3. You need 2 different versions of the same library for different applications.


    - The solution is for each application to use its own library directory that contains all of an 
        application's dependencies.  This directory is known as a 'virtual environment'.



- Setting Up a Virtual Environment

    - The 'virtualenv' tool handles virtual environments automatically for you.  

        # Create virtual environment
        $ python3 -m venv myvenv

        # Activate the virtual environment
        $ source myenv/bin/activate

        # Deactivate the virtual environment
        $ deactivate



- Using virtualenv with tox

    - The 'tox' management tool aims to automate and standardize how tests are run in Python.
        It provides everything needed to run an entire test suite in a clean virtual environment,
        while also installing your application to check that the installation works.

        $ pip install tox


    - First, we need to create a config file 'tox.ini' in the root directory of our project, 
        beside the 'setup.py'.

        # Create a config file
        $ touch tox.ini

        # Run tox
        $ tox


    - In this instance, tox creates a virtual environment in .tox/python using the default 
        Python version.  It uses 'setup.py' to create a distribution of your package, which it
        then installs in this virtual environment.  


    - Next, we add a command to run to our 'tox.ini'.  We need to add pytest as a dependency
        that must be installed in our virutal environment.

        # tox.ini
        [testenv]
        deps=pytest
        commands=pytest


    - Now, when we run 'tox', tox recreates the environment, installs the new dependency, and
        runs the command 'pytest', which executes all the unit tests.
        


- Re-creating an Environment


- Using Different Python Versions


- Integrating Other Tests


- Testing Policy